import os
import time
import random
import json
import re
from playwright.sync_api import sync_playwright

# å…¨å±€å˜é‡ï¼Œç¡®ä¿æ•°æ®ä¸ä¼šä¸¢å¤±
GLOBAL_PRODUCTS = []

def handle_response(response):
    global GLOBAL_PRODUCTS
    
    # æ”¾å®½æ‹¦æˆªæ¡ä»¶ï¼šåªè¦æ˜¯ API è¯·æ±‚æˆ–è€…åŒ…å« search å…³é”®å­—
    # æ’é™¤å›¾ç‰‡ã€CSSã€JS ç­‰é™æ€èµ„æº
    resource_type = response.request.resource_type
    if resource_type in ["xhr", "fetch", "script"]:
        try:
            # æ£€æŸ¥ Content-Type
            content_type = response.headers.get("content-type", "")
            if "json" in content_type or "javascript" in content_type:
                text = response.text()
                
                # å…³é”®ç‰¹å¾åŒ¹é…ï¼šæ·˜å®æœç´¢ç»“æœé€šå¸¸åŒ…å« raw_title æˆ– view_price
                if '"raw_title"' in text or '"view_price"' in text or '"title":' in text:
                    # æ’é™¤ä¸€äº›æ— å…³çš„ API
                    if "suggest" in response.url: return 

                    print(f"   âš¡ æ•è·åˆ°ç–‘ä¼¼å•†å“æ•°æ®: {response.url[:60]}...")
                    
                    # å°è¯•å¤šç§å­—æ®µååŒ¹é…
                    # æ–¹æ¡ˆ A: raw_title (å¸¸è§äº mtop æ¥å£)
                    titles = re.findall(r'"raw_title":"([^"]+)"', text)
                    if not titles:
                        # æ–¹æ¡ˆ B: title (å¸¸è§äº pc æ¥å£)
                        titles = re.findall(r'"title":"([^"]+)"', text)
                    
                    prices = re.findall(r'"view_price":"([^"]+)"', text)
                    nids = re.findall(r'"nid":"([^"]+)"', text)
                    
                    # é”€é‡ (view_sales)
                    sales = re.findall(r'"view_sales":"([^"]+)"', text)
                    
                    # åº—é“ºå (nick)
                    shops = re.findall(r'"nick":"([^"]+)"', text)

                    if titles and len(titles) > 0:
                        print(f"   âœ… æˆåŠŸæå–åˆ° {len(titles)} æ¡è®°å½•")
                        for i in range(len(titles)):
                            # å°½å¯èƒ½å¤šåœ°åŒ¹é…å­—æ®µ
                            price = prices[i] if i < len(prices) else "æœªçŸ¥"
                            nid = nids[i] if i < len(nids) else ""
                            sale = sales[i] if i < len(sales) else "0"
                            shop = shops[i] if i < len(shops) else "æœªçŸ¥åº—é“º"
                            
                            if nid and not any(p['id'] == nid for p in GLOBAL_PRODUCTS):
                                GLOBAL_PRODUCTS.append({
                                    "id": nid,
                                    "title": titles[i],
                                    "price": price,
                                    "link": f"https://item.taobao.com/item.htm?id={nid}",
                                    "shop": shop,
                                    "deal_count": sale
                                })
                        print(f"   ğŸ“ˆ å½“å‰å…¨å±€åˆ—è¡¨æ€»æ•°: {len(GLOBAL_PRODUCTS)}")
        except Exception as e:
            pass # å¿½ç•¥è§£æé”™è¯¯
        except Exception as e:
            pass # å¿½ç•¥è§£æé”™è¯¯

def run_scraper(keyword=None, max_pages=None):
    global GLOBAL_PRODUCTS
    GLOBAL_PRODUCTS = [] # é‡ç½®
    
    with sync_playwright() as p:
        # å¯åŠ¨æœ‰å¤´æµè§ˆå™¨ï¼Œæ–¹ä¾¿ç”¨æˆ·æ‰«ç 
        # æ·»åŠ åçˆ¬è™«ç»•è¿‡å‚æ•°
        browser = p.chromium.launch(
            headless=False,
            args=["--disable-blink-features=AutomationControlled"]
        )
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={"width": 1280, "height": 800}
        )
        
        # æ³¨å…¥è„šæœ¬ä»¥è¿›ä¸€æ­¥éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
        page = context.new_page()
        page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)

        # å¼€å¯ç›‘å¬
        page.on("response", handle_response)

        print("ğŸš€ æ­£åœ¨æ‰“å¼€æ·˜å®é¦–é¡µï¼Œè¯·å‡†å¤‡æ‰«ç ç™»å½•...")
        page.goto("https://www.taobao.com")
        
        # ç­‰å¾…ç”¨æˆ·ç™»å½•
        print("ğŸ”” [è¯·æ³¨æ„]ï¼šæ·˜å®å¿…é¡»ç™»å½•æ‰èƒ½æŸ¥çœ‹è¯¦æƒ…ã€‚")
        print("ğŸ‘‰ è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­å®Œæˆæ‰«ç ç™»å½•ã€‚")
        # åªæœ‰åœ¨æ²¡æœ‰æä¾›å…³é”®è¯æ—¶æ‰æš‚åœç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼Œæˆ–è€…å¦‚æœæä¾›äº†å…³é”®è¯ä½†ä¸ºäº†ç¡®ä¿ç™»å½•æˆåŠŸï¼Œä¹Ÿå¯ä»¥ç­‰å¾…
        # ä¸ºäº†è‡ªåŠ¨åŒ–æµç•…ï¼Œå¦‚æœæä¾›äº†å…³é”®è¯ï¼Œæˆ‘ä»¬å‡è®¾ç”¨æˆ·ä¼šåœ¨è„šæœ¬å¯åŠ¨åçš„çŸ­æ—¶é—´å†…å®Œæˆç™»å½•ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥æ£€æµ‹ç™»å½•çŠ¶æ€
        # è¿™é‡Œä¿ç•™æ‰‹åŠ¨ç¡®è®¤ï¼Œå› ä¸ºæ‰«ç ç™»å½•å¾ˆéš¾è‡ªåŠ¨åŒ–æ£€æµ‹å®Œå…¨
        input("âœ… ç™»å½•å®Œæˆåï¼Œè¯·åœ¨æ§åˆ¶å°æŒ‰ [å›è½¦] é”®ç»§ç»­...")

        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs("data", exist_ok=True)

        # æ”¹ä¸ºè¾“å…¥å…³é”®è¯ï¼Œè¿›è¡Œæ‰¹é‡æœç´¢
        if not keyword:
            keyword = input("ğŸ” è¯·è¾“å…¥æœç´¢å…³é”®è¯ (ä¾‹å¦‚ 'æœºæ¢°é”®ç›˜'): ").strip()
        
        if not keyword:
            print("âŒ å…³é”®è¯ä¸èƒ½ä¸ºç©ºï¼")
            return
            
        if not max_pages:
            max_pages_input = input("ğŸ“„ è¯·è¾“å…¥è¦æŠ“å–çš„é¡µæ•° (é»˜è®¤ 5): ").strip()
            max_pages = int(max_pages_input) if max_pages_input.isdigit() else 5

        for page_num in range(1, max_pages + 1):
            # æ„é€ æ·˜å®æœç´¢é“¾æ¥ (æ·˜å®æœç´¢é¡µç¿»é¡µé€šå¸¸æ˜¯ s å‚æ•°ï¼Œæ¯é¡µ 44 ä¸ªå•†å“)
            # page 1: s=0, page 2: s=44, page 3: s=88
            offset = (page_num - 1) * 44
            search_url = f"https://s.taobao.com/search?q={keyword}&s={offset}"
            
            print(f"ğŸš€ [ç¬¬ {page_num}/{max_pages} é¡µ] æ­£åœ¨è®¿é—®: {search_url}")
            
            try:
                page.goto(search_url, timeout=60000)
                page.wait_for_load_state("domcontentloaded")
                time.sleep(random.uniform(3, 5)) # ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½
                
                # æ¨¡æ‹Ÿæ»šåŠ¨åˆ°åº•éƒ¨ä»¥è§¦å‘æ‡’åŠ è½½ (è¿™ä¼šè§¦å‘æ›´å¤š API è¯·æ±‚)
                for _ in range(5):
                    page.mouse.wheel(0, 1000)
                    time.sleep(1)
                
                # éšæœºç­‰å¾…ï¼Œé˜²å°
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                print(f"   âŒ æœ¬é¡µæŠ“å–å¤±è´¥: {e}")

        # ç§»é™¤ç›‘å¬
        page.remove_listener("response", handle_response)

        # ä¿å­˜ç»“æœ
        output_file = "data/search_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(GLOBAL_PRODUCTS, f, ensure_ascii=False, indent=2)
            
        print(f"\nğŸ‰ æµ·é‡æŠ“å–ç»“æŸï¼å…±æ”¶é›† {len(GLOBAL_PRODUCTS)} ä¸ªå•†å“ä¿¡æ¯ã€‚")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        print("ğŸ‘‰ ä¸‹ä¸€æ­¥ï¼šè¯·è¿è¡Œåˆ†æå™¨è¿›è¡Œåˆç­›ã€‚")
        
        browser.close()

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨è¯¦æƒ…é¡µæŠ“å–åˆ°çš„æ•°æ®
GLOBAL_DETAILS = {}

def handle_detail_response(response):
    """
    ä¸“é—¨å¤„ç†è¯¦æƒ…é¡µçš„ API å“åº”
    """
    global GLOBAL_DETAILS
    
    try:
        url = response.url
        # æ‹¦æˆªè¯„è®ºæ¥å£ (rate) å’Œè¯¦æƒ…æ¥å£ (detail)
        if "rate" in url or "detail" in url or "mtop" in url:
            content_type = response.headers.get("content-type", "")
            if "json" in content_type or "javascript" in content_type:
                text = response.text()
                
                # æå– JSONP ä¸­çš„ JSON
                if text.strip().startswith("mtopjsonp") or text.strip().startswith("jsonp"):
                    match = re.search(r'\((.*)\)', text)
                    if match:
                        text = match.group(1)
                
                # å°è¯•è§£æ JSON
                try:
                    data = json.loads(text)
                    
                    # è¯†åˆ«è¯„è®ºæ•°æ®
                    if "rateList" in text or "rateDetail" in text:
                        # æ‰¾åˆ°å½“å‰é¡µé¢çš„å•†å“ ID (ä» URL æˆ– Referer ä¸­æ¨æ–­ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾åªæœ‰ä¸€ä¸ªé¡µé¢åœ¨æ´»åŠ¨)
                        # æ›´å¥½çš„æ–¹å¼æ˜¯æŠŠæ•°æ®æš‚å­˜ï¼Œæœ€åç»Ÿä¸€å…³è”
                        # è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°æŠŠæ‰€æœ‰æ•è·åˆ°çš„ rateList å­˜èµ·æ¥
                        if "rateList" not in GLOBAL_DETAILS:
                            GLOBAL_DETAILS["rateList"] = []
                        
                        # æå–è¯„è®ºåˆ—è¡¨
                        # ç»“æ„é€šå¸¸æ˜¯ data -> rateDetail -> rateList
                        rate_list = data.get("data", {}).get("rateDetail", {}).get("rateList", [])
                        if rate_list:
                            print(f"   ğŸ’¬ æ•è·åˆ° {len(rate_list)} æ¡è¯„è®ºæ•°æ®")
                            GLOBAL_DETAILS["rateList"].extend(rate_list)

                    # è¯†åˆ«å•†å“å‚æ•°æ•°æ®
                    if "item" in text and "props" in text:
                         if "itemProps" not in GLOBAL_DETAILS:
                             GLOBAL_DETAILS["itemProps"] = {}
                         
                         # å°è¯•æå– props
                         props = data.get("data", {}).get("item", {}).get("props", [])
                         if props:
                             print(f"   ğŸ“ æ•è·åˆ°å•†å“å‚æ•°æ•°æ®")
                             GLOBAL_DETAILS["itemProps"] = props

                except:
                    pass
    except:
        pass

def run_deep_scraper():
    """
    ç¬¬ä¸‰é˜¶æ®µï¼šç²¾å‡†æ·±åº¦é‡‡é›† (å‡çº§ç‰ˆï¼šä½¿ç”¨ç½‘ç»œæ‹¦æˆª)
    è¯»å– data/top_candidates.jsonï¼ŒæŠ“å–è¯¦æƒ…é¡µ
    """
    global GLOBAL_DETAILS
    
    input_file = "data/top_candidates.json"
    if not os.path.exists(input_file):
        print(f"âŒ æ–‡ä»¶ {input_file} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œåˆç­›ã€‚")
        return

    with open(input_file, "r", encoding="utf-8") as f:
        candidates = json.load(f)

    if not candidates:
        print("âš ï¸ å€™é€‰åˆ—è¡¨ä¸ºç©ºã€‚")
        return

    print(f"ğŸš€ å¼€å§‹æ·±åº¦é‡‡é›† {len(candidates)} ä¸ªç²¾é€‰å•†å“ (ç½‘ç»œæ‹¦æˆªæ¨¡å¼)...")
    os.makedirs("data/details", exist_ok=True)

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=False,
            args=["--disable-blink-features=AutomationControlled"]
        )
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={"width": 1280, "height": 800}
        )
        page = context.new_page()
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        # å¼€å¯ç›‘å¬
        page.on("response", handle_detail_response)

        # ç™»å½•æ£€æŸ¥
        print("ğŸš€ æ­£åœ¨æ‰“å¼€æ·˜å®é¦–é¡µï¼Œè¯·å‡†å¤‡æ‰«ç ç™»å½•...")
        page.goto("https://www.taobao.com")
        print("ğŸ”” [è¯·æ³¨æ„]ï¼šæ·˜å®å¿…é¡»ç™»å½•æ‰èƒ½æŸ¥çœ‹è¯¦æƒ…ã€‚")
        print("ğŸ‘‰ è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­å®Œæˆæ‰«ç ç™»å½•ã€‚")
        input("âœ… ç™»å½•å®Œæˆåï¼Œè¯·åœ¨æ§åˆ¶å°æŒ‰ [å›è½¦] é”®ç»§ç»­...")

        for i, item in enumerate(candidates):
            # é‡ç½®å½“å‰å•†å“çš„æŠ“å–æ•°æ®
            GLOBAL_DETAILS = {"rateList": [], "itemProps": []}
            
            url = item['link']
            print(f"ğŸ”„ [{i+1}/{len(candidates)}] æ­£åœ¨æ·±åº¦æŠ“å–: {item['title'][:20]}...")
            
            try:
                if not url.startswith("http"):
                    url = "https:" + url
                
                page.goto(url, timeout=60000)
                page.wait_for_load_state("domcontentloaded")
                
                # æ¨¡æ‹Ÿæ·±åº¦æµè§ˆ
                print("   æ­£åœ¨åŠ è½½è¯¦æƒ…å’Œè¯„è®º (è§¦å‘ API)...")
                for _ in range(5):
                    page.mouse.wheel(0, 1000)
                    time.sleep(1.5)
                
                # å°è¯•ç‚¹å‡»â€œç´¯è®¡è¯„ä»·â€
                try:
                    page.click("text=ç´¯è®¡è¯„ä»·", timeout=2000)
                    time.sleep(3) # ç­‰å¾…è¯„è®º API åŠ è½½
                except:
                    pass
                
                # ä¿å­˜æŠ“å–åˆ°çš„ JSON æ•°æ®ï¼Œè€Œä¸æ˜¯ HTML
                detail_data = {
                    "id": item['id'],
                    "title": item['title'],
                    "price": item['price'],
                    "shop": item['shop'],
                    "captured_reviews": GLOBAL_DETAILS.get("rateList", []),
                    "captured_props": GLOBAL_DETAILS.get("itemProps", [])
                }
                
                file_name = f"data/details/{item['id']}.json"
                with open(file_name, "w", encoding="utf-8") as f:
                    json.dump(detail_data, f, ensure_ascii=False, indent=2)
                
                print(f"   âœ… å·²ä¿å­˜è¯¦æƒ…æ•°æ®: {file_name} (è¯„è®ºæ•°: {len(detail_data['captured_reviews'])})")
                
                time.sleep(random.uniform(3, 5))
                
            except Exception as e:
                print(f"   âŒ æŠ“å–å¤±è´¥: {e}")

        print("ğŸ‰ æ·±åº¦é‡‡é›†ç»“æŸï¼")
        browser.close()

if __name__ == "__main__":
    print("1. è¿è¡Œæµ·é‡åˆ—è¡¨æŠ“å– (Scraper)")
    print("2. è¿è¡Œç²¾å‡†æ·±åº¦é‡‡é›† (Deep Scraper - éœ€å…ˆæœ‰ top_candidates.json)")
    choice = input("è¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        run_scraper()
    elif choice == "2":
        run_deep_scraper()
    else:
        print("æ— æ•ˆé€‰æ‹©")
