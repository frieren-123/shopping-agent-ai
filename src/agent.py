import os
import shutil
import json
import asyncio
from src.scrapers.taobao import TaobaoScraper
from src.scrapers.jd_gui import JDScraper # âœ… è§†è§‰ OCR çˆ¬è™«
from src.scrapers.jd_crawl4ai import JDCrawl4AIScraper # âœ… AI å¢å¼ºç‰ˆçˆ¬è™«
from src.scrapers.vip import VipScraper
from src.scrapers.zhihu import ZhihuScraper
from src.analysis.scorer import SmartScorer
from src.llm_analyzer import filter_products, analyze_products, ask_clarifying_questions
from src.config_loader import CONFIG
from src.report_engine import ReportEngine # âœ… æ–°å¢æŠ¥å‘Šå¼•æ“

class ShoppingAgent:
    def __init__(self):
        self.config = CONFIG
        self.products = []
        self.top_candidates = []
        self.reporter = ReportEngine() # âœ… åˆå§‹åŒ–æŠ¥å‘Šå¼•æ“

    def clean_data(self):
        """æ¸…ç†æ—§æ•°æ®ï¼Œä¸ºæ–°ä»»åŠ¡åšå‡†å¤‡"""
        print("ğŸ§¹ æ­£åœ¨æ¸…ç†æ—§æ•°æ®...")
        if os.path.exists("data"):
            for filename in os.listdir("data"):
                file_path = os.path.join("data", filename)
                try:
                    if os.path.isfile(file_path) or os.path.islink(file_path):
                        os.unlink(file_path)
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤ {file_path} å¤±è´¥: {e}")
        else:
            os.makedirs("data")
        
        os.makedirs("data/details", exist_ok=True)
        print("âœ… æ•°æ®æ¸…ç†å®Œæˆ")

    def ask_clarifying_questions(self, keyword):
        return ask_clarifying_questions(keyword)

    def search(self, keyword, max_pages=None, platform_choice="1"):
        """åŒæ­¥å…¥å£ (å…¼å®¹æ—§ä»£ç )"""
        import sys
        # ä¿®å¤ Windows ä¸‹ Playwright çš„ NotImplementedError
        if sys.platform == 'win32':
            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
            
        return asyncio.run(self.search_async(keyword, max_pages, platform_choice))

    async def search_async(self, keyword, max_pages=None, platform_choice="1"):
        """å¼‚æ­¥æœç´¢æ ¸å¿ƒé€»è¾‘"""
        if max_pages is None:
            max_pages = self.config["crawler"]["max_pages"]
        
        self.products = []
        refined_keyword = keyword 

        tasks = []

        # æ·˜å® (ç›®å‰ä»ä¸ºåŒæ­¥ï¼Œæ”¾å…¥çº¿ç¨‹æ± æˆ–ç›´æ¥è°ƒç”¨)
        if platform_choice == "2" or platform_choice == "4":
            print("\nğŸ“¦ æ­£åœ¨å¯åŠ¨æ·˜å®æŠ“å–...")
            # ç®€å•çš„åŒæ­¥è°ƒç”¨åŒ…è£…
            try:
                taobao_scraper = TaobaoScraper()
                # æ³¨æ„ï¼šè¿™é‡Œå¦‚æœæ˜¯è€—æ—¶æ“ä½œï¼Œå»ºè®®ç”¨ run_in_executorï¼Œä½†ä¸ºäº†ç®€å•å…ˆç›´æ¥è°ƒç”¨
                tb_products = taobao_scraper.search(keyword=refined_keyword, max_pages=max_pages)
                self.products.extend(tb_products)
            except Exception as e:
                print(f"âš ï¸ æ·˜å®æŠ“å–å¤±è´¥: {e}")

        # å”¯å“ä¼š
        if platform_choice == "3" or platform_choice == "4":
            print("\nğŸ›ï¸ æ­£åœ¨å¯åŠ¨å”¯å“ä¼šæŠ“å–...")
            try:
                vip_scraper = VipScraper()
                vip_products = vip_scraper.search(keyword=refined_keyword, max_pages=max_pages)
                self.products.extend(vip_products)
            except Exception as e:
                print(f"âš ï¸ å”¯å“ä¼šæŠ“å–å¤±è´¥: {e}")

        # äº¬ä¸œ (é»˜è®¤æˆ–å…¨é€‰) - åˆ‡æ¢ä¸º Crawl4AI å¼‚æ­¥ç‰ˆ
        if platform_choice == "1" or platform_choice == "4" or platform_choice == "5" or (platform_choice not in ["2", "3", "4", "5", "6"]):
            print("\nğŸ¤– æ­£åœ¨å¯åŠ¨äº¬ä¸œ AI å¢å¼ºç‰ˆæŠ“å– (Crawl4AI)...")
            try:
                ai_scraper = JDCrawl4AIScraper()
                # ç›´æ¥ await å¼‚æ­¥æ–¹æ³•
                jd_products = await ai_scraper.search(keyword=refined_keyword, max_pages=max_pages)
                self.products.extend(jd_products)
            except Exception as e:
                print(f"âš ï¸ äº¬ä¸œ AI æŠ“å–å¤±è´¥: {e}")

        # äº¬ä¸œ OCR ç‰ˆ
        if platform_choice == "6":
            print("\nğŸ“¸ æ­£åœ¨å¯åŠ¨äº¬ä¸œè§†è§‰ OCR æŠ“å– (PaddleOCR)...")
            try:
                # OCR çˆ¬è™«ç›®å‰æ˜¯åŒæ­¥çš„ï¼Œç›´æ¥è°ƒç”¨
                ocr_scraper = JDScraper()
                jd_products = ocr_scraper.search(keyword=refined_keyword, max_pages=max_pages)
                self.products.extend(jd_products)
            except Exception as e:
                print(f"âš ï¸ OCR æŠ“å–å¤±è´¥: {e}")
        
        # æ™ºèƒ½æ‰“åˆ†ä¸æ’åº
        if self.products:
            print("\nğŸ§® æ­£åœ¨åº”ç”¨æ™ºèƒ½æ‰“åˆ†ç®—æ³• (Bayesian + Z-Score)...")
            scorer = SmartScorer(self.products)
            self.products = scorer.rank_products()
            
            # âœ… ä½¿ç”¨æ–°çš„æŠ¥å‘Šå¼•æ“æ‰“å° CLI æ‘˜è¦
            self.reporter.print_cli_summary(self.products[:10])
            
            # ä¿å­˜ç»“æœ
            output_file = "data/search_results.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(self.products, f, ensure_ascii=False, indent=2)
        
        return self.products

    def filter_products(self, detailed_requirements, top_n=None):
        if top_n is None:
            top_n = self.config["filter"]["top_n"]
            
        self.top_candidates = filter_products(detailed_requirements, top_n=top_n)
        return self.top_candidates

    def get_details(self):
        # åˆ†ç¦»ä¸åŒå¹³å°çš„å•†å“
        tb_candidates = [p for p in self.top_candidates if p.get('platform') not in ['JD', 'Vipshop']]
        other_candidates = [p for p in self.top_candidates if p.get('platform') in ['JD', 'Vipshop']]
        
        if tb_candidates:
            print(f"ğŸ“¦ æ­£åœ¨é‡‡é›† {len(tb_candidates)} ä¸ªæ·˜å®å•†å“è¯¦æƒ…...")
            try:
                taobao_scraper = TaobaoScraper()
                taobao_scraper.get_details(tb_candidates)
            except Exception as e:
                print(f"âš ï¸ æ·˜å®è¯¦æƒ…é‡‡é›†å¤±è´¥: {e}")
            
        if other_candidates:
            print(f"âœ¨ å…¶ä»–å¹³å°å•†å“ ({len(other_candidates)} ä¸ª) å·²åœ¨æœç´¢é˜¶æ®µè·å–äº†è¶³å¤Ÿä¿¡æ¯ï¼Œè·³è¿‡æ·±åº¦é‡‡é›†ã€‚")
            os.makedirs("data/details", exist_ok=True)
            for item in other_candidates:
                platform_name = "äº¬ä¸œ" if item.get('platform') == 'JD' else "å”¯å“ä¼š"
                detail_data = {
                    "id": item['id'],
                    "title": item['title'],
                    "price": item['price'],
                    "shop": item['shop'],
                    "captured_reviews": [{"content": f"{platform_name}çƒ­åº¦/é”€é‡: " + str(item.get('deal_count', 'æœªçŸ¥'))}], 
                    "captured_props": [{"name": "æ¥æº", "value": f"{platform_name}è‡ªè¥/å“ç‰Œåº—"}]
                }
                with open(f"data/details/{item['id']}.json", "w", encoding="utf-8") as f:
                    json.dump(detail_data, f, ensure_ascii=False, indent=2)

    def analyze_products(self):
        return analyze_products()

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            if os.path.exists("data/details"):
                shutil.rmtree("data/details")
            if os.path.exists("data/search_results.json"):
                os.remove("data/search_results.json")
            if os.path.exists("data/top_candidates.json"):
                os.remove("data/top_candidates.json")
            print("âœ… ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å¤±è´¥: {e}")
