import asyncio
import re
import sys
import os

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
try:
    from src.llm_analyzer import extract_info_from_markdown
except ImportError:
    # Fallback if import fails
    def extract_info_from_markdown(text):
        print("   âš ï¸ æ— æ³•å¯¼å…¥ LLM æå–å™¨ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨")
        return []

class JDCrawl4AIScraper:
    def __init__(self):
        pass

    def search_sync(self, keyword, max_pages=1):
        """åŒæ­¥åŒ…è£…å™¨ï¼Œæ–¹ä¾¿ main.py è°ƒç”¨"""
        return asyncio.run(self.search(keyword, max_pages))

    async def search(self, keyword, max_pages=1):
        results = []
        print(f"ğŸš€ [Crawl4AI] å¯åŠ¨æ™ºèƒ½æœç´¢: {keyword}")
        
        # 1. é…ç½®æµè§ˆå™¨ (ä½¿ç”¨ç‹¬ç«‹çš„ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œä¸å½±å“æ—¥å¸¸ä½¿ç”¨)
        # æˆ‘ä»¬ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ç›®å½•ï¼Œä¸“é—¨ç»™çˆ¬è™«ç”¨ï¼Œä½†ä½¿ç”¨ Edge å†…æ ¸
        
        # åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„çˆ¬è™«ä¸“ç”¨ç›®å½•
        crawler_data_dir = os.path.join(os.getcwd(), "browser_data_edge_crawler")
        if not os.path.exists(crawler_data_dir):
            os.makedirs(crawler_data_dir, exist_ok=True)

        print(f"   ğŸŒ ä½¿ç”¨ç‹¬ç«‹ç¯å¢ƒ (ä¸å½±å“æ—¥å¸¸æµè§ˆå™¨)...")
        print(f"   ğŸ“‚ æ•°æ®ç›®å½•: {crawler_data_dir}")

        browser_config = BrowserConfig(
            headless=False, 
            verbose=True,
            browser_type="chromium",
            user_data_dir=crawler_data_dir,
            # âœ… å¢åŠ åçˆ¬è™«ç­–ç•¥ (Stealth)
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            },
            # å°è¯•ä¼ é€’ç»™ Playwright çš„å¯åŠ¨å‚æ•°
            # args=["--disable-blink-features=AutomationControlled", "--no-sandbox"] 
        )
        
        # 2. å®šä¹‰æ»šåŠ¨è„šæœ¬ (äº¬ä¸œæ˜¯æ‡’åŠ è½½ï¼Œå¿…é¡»æ»šåŠ¨åˆ°åº•éƒ¨)
        js_code = """
            const scrollStep = 500;
            const delay = 500;
            let lastHeight = document.body.scrollHeight;
            
            while (true) {
                window.scrollBy(0, scrollStep);
                await new Promise(resolve => setTimeout(resolve, delay));
                
                let newHeight = document.body.scrollHeight;
                if ((window.innerHeight + window.scrollY) >= document.body.offsetHeight) {
                    break;
                }
            }
        """

        run_config = CrawlerRunConfig(
            js_code=js_code,
            cache_mode=CacheMode.BYPASS,
            page_timeout=60000,
            # âœ… å¢åŠ éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            delay_before_return_html=2.0,
            # âœ… å…³é”®ï¼šç­‰å¾…å•†å“åˆ—è¡¨å…ƒç´ åŠ è½½ï¼Œç¡®ä¿ä¸æ˜¯ç©ºé¡µé¢æˆ–é¦–é¡µ
            wait_for="css:#J_goodsList", 
        )

        async with AsyncWebCrawler(config=browser_config) as crawler:
            for page in range(1, max_pages + 1):
                try:
                    # äº¬ä¸œåˆ†é¡µé€»è¾‘
                    jd_page_param = 2 * page - 1
                    url = f"https://search.jd.com/Search?keyword={keyword}&page={jd_page_param}"
                    
                    print(f"   ğŸ”„ [ç¬¬ {page} é¡µ] AI æ­£åœ¨é˜…è¯»é¡µé¢...")
                    print("   â³ å¦‚æœå¼¹å‡ºç™»å½•çª—å£ï¼Œè¯·åœ¨ 60ç§’å†… å®Œæˆæ‰«ç ç™»å½•...")
                    
                    # æ™ºèƒ½é‡è¯•å¾ªç¯ï¼šå¤„ç†ç™»å½•
                    max_retries = 12 # 12 * 5s = 60ç§’ç­‰å¾…æ—¶é—´
                    is_login = False
                    result = None
                    
                    for attempt in range(max_retries):
                        try:
                            result = await crawler.arun(url=url, config=run_config)
                            
                            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°äº†é¦–é¡µ
                            if result.url and "www.jd.com" in result.url and "search" not in result.url:
                                print("   ğŸš¨ æ£€æµ‹åˆ°è¢«é‡å®šå‘å›é¦–é¡µï¼Œå¯èƒ½æ˜¯åçˆ¬è™«æ‹¦æˆªï¼")
                                print("   ğŸ‘‰ è¯·å°è¯•æ‰‹åŠ¨åœ¨æµè§ˆå™¨ä¸­æœç´¢ä¸€æ¬¡ï¼Œæˆ–æ£€æŸ¥ç™»å½•çŠ¶æ€ã€‚")
                                is_login = True # è§†ä¸ºç™»å½•/éªŒè¯å¤±è´¥
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ç™»å½•é¡µ (å†…å®¹è¿‡çŸ­ æˆ– åŒ…å«ç‰¹å®šå…³é”®è¯)
                            elif result.markdown:
                                is_login = len(result.markdown) < 800 or "è¯·ç™»å½•" in result.markdown or "æ‰«ç ç™»å½•" in result.markdown
                            else:
                                is_login = True
                            
                            if is_login:
                                print(f"   ğŸš¨ [ç¬¬ {attempt+1}/{max_retries} æ¬¡æ£€æµ‹] ä¼¼ä¹è¿˜åœ¨ç™»å½•é¡µæˆ–é¦–é¡µï¼Œè¯·æ‰«ç /éªŒè¯...")
                                print("      (ç™»å½•æˆåŠŸåï¼Œç¨‹åºä¼šè‡ªåŠ¨è·³è½¬ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ)")
                                await asyncio.sleep(5) # ç­‰å¾…ç”¨æˆ·æ“ä½œ
                            else:
                                # æˆåŠŸè·å–åˆ°å†…å®¹
                                break
                        except Exception as e:
                            print(f"   âš ï¸ å°è¯•è¯»å–å¤±è´¥: {e}")
                            await asyncio.sleep(5)
                    
                    if result and result.success and not is_login:
                        print(f"   âœ… é¡µé¢è¯»å–æˆåŠŸ (é•¿åº¦: {len(result.markdown)} å­—ç¬¦)")
                        
                        # å¦‚æœé¡µé¢å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯è¢«éªŒè¯ç æ‹¦æˆªäº†
                        if len(result.markdown) < 5000:
                            print("   âš ï¸ è­¦å‘Šï¼šé¡µé¢å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½è§¦å‘äº†éªŒè¯ç æˆ–æœªåŠ è½½æˆåŠŸã€‚")
                            print(f"   ğŸ‘€ é¡µé¢é¢„è§ˆ: {result.markdown[:200]}...")
                        
                        # --- æ™ºèƒ½è§£æ (ä½¿ç”¨ LLM æå–æ•°æ®) ---
                        print("   ğŸ§  æ­£åœ¨è°ƒç”¨ DeepSeek/GPT æå–å•†å“ä¿¡æ¯...")
                        # ä½¿ç”¨ to_thread é¿å…é˜»å¡å¼‚æ­¥å¾ªç¯
                        items = await asyncio.to_thread(extract_info_from_markdown, result.markdown)
                        
                        print(f"   ğŸ“„ æœ¬é¡µæå–åˆ° {len(items)} ä¸ªå•†å“")
                        
                        if len(items) == 0:
                            print("   âš ï¸ è­¦å‘Šï¼šæœªæå–åˆ°å•†å“ï¼Œæ­£åœ¨ä¿å­˜è°ƒè¯•æ–‡ä»¶...")
                            with open("debug_jd_markdown.md", "w", encoding="utf-8") as f:
                                f.write(result.markdown)
                            print("   ğŸ’¾ é¡µé¢ Markdown å·²ä¿å­˜è‡³ debug_jd_markdown.md")

                        results.extend(items)
                        
                    else:
                        err_msg = result.error_message if result else "Unknown Error"
                        print(f"   âŒ æŠ“å–å¤±è´¥: {err_msg}")
                
                except Exception as e:
                    print(f"   âŒ é¡µé¢ {page} å¤„ç†å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                    continue # ç»§ç»­ä¸‹ä¸€é¡µ
                    
        print("   ğŸ›‘ æ­£åœ¨å…³é—­çˆ¬è™«æµè§ˆå™¨...")
        # async with å—ç»“æŸæ—¶ä¼šè‡ªåŠ¨å…³é—­æµè§ˆå™¨ï¼Œè¿™é‡Œåªæ˜¯æ‰“å°çŠ¶æ€
        
        return results
